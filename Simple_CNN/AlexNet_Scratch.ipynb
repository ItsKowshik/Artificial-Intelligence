{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4DsEFSAZUWCD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "PONs5SZNUqaG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZJ8Kxp0U7cr",
        "outputId": "fcac7d73-248c-4dd0-a43f-08ceb5344ca2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the train and valid preprocessing code\n",
        "def train_valid_loader(data_dir,\n",
        "                       batch_size,\n",
        "                       augment,\n",
        "                       random_seed,\n",
        "                       valid_size=0.1,\n",
        "                       shuffle = True):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean = [0.4914, 0.4822, 0.4465],\n",
        "        std = [0.2023, 0.1994, 0.2010]\n",
        "    )\n",
        "    #Define required transforms\n",
        "    valid_transform = transforms.Compose([\n",
        "        transforms.Resize((227, 227)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "    if augment:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(32,padding = 4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((227, 227)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "    #Load the dataset\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root = data_dir,\n",
        "        train = True,\n",
        "        download = True,\n",
        "        transform = train_transform,)\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root = data_dir,\n",
        "        train = True,\n",
        "        download = True,\n",
        "        transform = valid_transform,)\n",
        "\n",
        "    #Get parameters\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    #Shuffle indices\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    #Split indices\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    #Sampler\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    #Loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size = batch_size,\n",
        "        sampler = train_sampler,)\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size = batch_size,\n",
        "        sampler = valid_sampler,)\n",
        "\n",
        "    return (train_loader, valid_loader)\n"
      ],
      "metadata": {
        "id": "jUlcKYdDU929"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loader(data_dir, batch_size,shuffle=True):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean = [0.4914, 0.4822, 0.4465],\n",
        "        std = [0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    #Define transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((227, 227)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,])\n",
        "\n",
        "    #Create dataset\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root = data_dir,\n",
        "        train = False,\n",
        "        download = True,\n",
        "        transform = transform,)\n",
        "\n",
        "    #Create dataloader\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size = batch_size,\n",
        "        shuffle = shuffle,)\n",
        "\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "hHDt3mqoXiYL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader = train_valid_loader(\n",
        "    data_dir = 'data',\n",
        "    batch_size = 64,\n",
        "    augment = False,\n",
        "    random_seed = 42,)\n",
        "\n",
        "test_loader = test_loader(\n",
        "    data_dir = 'data',\n",
        "    batch_size = 64,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4pAtipJYYNW",
        "outputId": "a85323b5-dc32-4f68-fd0a-6a446c3a98ce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create skeleton code for the class\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes = 10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Conv2d(3,96,kernel_size=11,stride=4,padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "        )\n",
        "        self.l2 = nn.Sequential(\n",
        "            nn.Conv2d(96,256,kernel_size=5,stride=1,padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "        )\n",
        "        self.l3 = nn.Sequential(\n",
        "            nn.Conv2d(256,384,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.l4 = nn.Sequential(\n",
        "            nn.Conv2d(384,384,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.l5 = nn.Sequential(\n",
        "            nn.Conv2d(384,256,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(9216,4096),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096,4096),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc2 = nn.Linear(4096,num_classes)\n",
        "\n",
        "    def forward(self,x):\n",
        "      out = self.l1(x)\n",
        "      out = self.l2(out)\n",
        "      out = self.l3(out)\n",
        "      out = self.l4(out)\n",
        "      out = self.l5(out)\n",
        "      out = out.reshape(out.size(0),-1)\n",
        "      out = self.fc(out)\n",
        "      out = self.fc1(out)\n",
        "      out = self.fc2(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "uRufxOHbYhvG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(AlexNet().to(device),input_size=(3,227,227))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoxC2JpEc494",
        "outputId": "92cfe59c-e4e2-4448-c250-a23f9282a900"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
            "       BatchNorm2d-2           [-1, 96, 55, 55]             192\n",
            "              ReLU-3           [-1, 96, 55, 55]               0\n",
            "         MaxPool2d-4           [-1, 96, 27, 27]               0\n",
            "            Conv2d-5          [-1, 256, 27, 27]         614,656\n",
            "       BatchNorm2d-6          [-1, 256, 27, 27]             512\n",
            "              ReLU-7          [-1, 256, 27, 27]               0\n",
            "         MaxPool2d-8          [-1, 256, 13, 13]               0\n",
            "            Conv2d-9          [-1, 384, 13, 13]         885,120\n",
            "      BatchNorm2d-10          [-1, 384, 13, 13]             768\n",
            "             ReLU-11          [-1, 384, 13, 13]               0\n",
            "           Conv2d-12          [-1, 384, 13, 13]       1,327,488\n",
            "      BatchNorm2d-13          [-1, 384, 13, 13]             768\n",
            "             ReLU-14          [-1, 384, 13, 13]               0\n",
            "           Conv2d-15          [-1, 256, 13, 13]         884,992\n",
            "      BatchNorm2d-16          [-1, 256, 13, 13]             512\n",
            "             ReLU-17          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-18            [-1, 256, 6, 6]               0\n",
            "          Dropout-19                 [-1, 9216]               0\n",
            "           Linear-20                 [-1, 4096]      37,752,832\n",
            "             ReLU-21                 [-1, 4096]               0\n",
            "          Dropout-22                 [-1, 4096]               0\n",
            "           Linear-23                 [-1, 4096]      16,781,312\n",
            "             ReLU-24                 [-1, 4096]               0\n",
            "           Linear-25                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 58,325,066\n",
            "Trainable params: 58,325,066\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.59\n",
            "Forward/backward pass size (MB): 16.04\n",
            "Params size (MB): 222.49\n",
            "Estimated Total Size (MB): 239.12\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "learning_rate = 0.005\n",
        "batch_size = 64\n",
        "\n",
        "model = AlexNet(num_classes).to(device)\n",
        "\n",
        "#Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9,weight_decay=0.005)\n",
        "\n",
        "#Train the model\n",
        "total_step = len(train_loader)\n",
        "loss_list = []\n",
        "val_acc_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i,(images,labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    #Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs,labels)\n",
        "\n",
        "    #Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "  loss_list.append(loss.item())\n",
        "\n",
        "  #Validation\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in valid_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        del images, labels, outputs\n",
        "    val_acc_list.append(100 * correct / total)\n",
        "    print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBgibynqa1z3",
        "outputId": "56b49e82-290f-4dfe-bc15-68ae4c2f2cb2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [704/704], Loss: 1.2861\n",
            "Accuracy of the network on the 5000 validation images: 60.16 %\n",
            "Epoch [2/20], Step [704/704], Loss: 1.0192\n",
            "Accuracy of the network on the 5000 validation images: 69.44 %\n",
            "Epoch [3/20], Step [704/704], Loss: 0.5147\n",
            "Accuracy of the network on the 5000 validation images: 72.44 %\n",
            "Epoch [4/20], Step [704/704], Loss: 1.0825\n",
            "Accuracy of the network on the 5000 validation images: 74.34 %\n",
            "Epoch [5/20], Step [704/704], Loss: 0.4349\n",
            "Accuracy of the network on the 5000 validation images: 77.5 %\n",
            "Epoch [6/20], Step [704/704], Loss: 0.5414\n",
            "Accuracy of the network on the 5000 validation images: 78.62 %\n",
            "Epoch [7/20], Step [704/704], Loss: 0.8947\n",
            "Accuracy of the network on the 5000 validation images: 78.74 %\n",
            "Epoch [8/20], Step [704/704], Loss: 0.9373\n",
            "Accuracy of the network on the 5000 validation images: 80.38 %\n",
            "Epoch [9/20], Step [704/704], Loss: 0.6834\n",
            "Accuracy of the network on the 5000 validation images: 79.84 %\n",
            "Epoch [10/20], Step [704/704], Loss: 1.1152\n",
            "Accuracy of the network on the 5000 validation images: 80.88 %\n",
            "Epoch [11/20], Step [704/704], Loss: 0.5590\n",
            "Accuracy of the network on the 5000 validation images: 81.98 %\n",
            "Epoch [12/20], Step [704/704], Loss: 0.7261\n",
            "Accuracy of the network on the 5000 validation images: 81.86 %\n",
            "Epoch [13/20], Step [704/704], Loss: 0.3848\n",
            "Accuracy of the network on the 5000 validation images: 82.68 %\n",
            "Epoch [14/20], Step [704/704], Loss: 0.4769\n",
            "Accuracy of the network on the 5000 validation images: 82.52 %\n",
            "Epoch [15/20], Step [704/704], Loss: 0.1828\n",
            "Accuracy of the network on the 5000 validation images: 83.62 %\n",
            "Epoch [16/20], Step [704/704], Loss: 0.2493\n",
            "Accuracy of the network on the 5000 validation images: 82.28 %\n",
            "Epoch [17/20], Step [704/704], Loss: 1.0144\n",
            "Accuracy of the network on the 5000 validation images: 82.4 %\n",
            "Epoch [18/20], Step [704/704], Loss: 0.2350\n",
            "Accuracy of the network on the 5000 validation images: 81.82 %\n",
            "Epoch [19/20], Step [704/704], Loss: 0.2898\n",
            "Accuracy of the network on the 5000 validation images: 83.52 %\n",
            "Epoch [20/20], Step [704/704], Loss: 0.9679\n",
            "Accuracy of the network on the 5000 validation images: 83.78 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddOwKRCIc9_1"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}